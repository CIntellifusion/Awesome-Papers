
# Generative Models
## Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval
RIT DongfengLiu 

### Motivation
Text content embedding is not sufficient to represent video in a semantic level. 

- 文本-视频检索任务面临的挑战在于如何准确地从大量候选视频中找到与文本查询语义最相关的视频片段。
- 现有的方法通常侧重于提取准确的视频或文本嵌入，但文本内容通常简短且简洁，难以充分描述视频的冗余语义。
- 因此，现有的单点文本嵌入可能不足以捕捉视频嵌入，导致检索效果不佳。

### Contribution

similarity-aware radius module to adapt the scale of text mass upon text-video pairs. 

text regularization to control text mass. 


1. **T-MASS模型**：提出了一种新的随机文本建模方法T-MASS（Text is Modeled As a Stochastic embedding），将文本建模为一个“质量块”（text mass），以增强文本嵌入的语义范围和鲁棒性。
2. **相似性感知半径模块**：引入了一个相似性感知半径模块，用于根据给定的文本-视频对调整文本质量块的规模。
3. **支持文本正则化**：设计了一种支持文本正则化方法，在训练过程中进一步控制文本质量块。



## Removing the Quality Tax in Controllable Face Generation
can be add to related work 

作者是 Yiwen Huang, Zhiqiu Yu, Xinjie Yi, Yue Wang, James Tompkin，来自布朗大学。以下是对论文动机、方法和实验的简要中文概述：/

### 动机：
- 论文探讨了在可控面部生成中存在的一个常见问题：为了实现对生成图像的明确控制（如身份、表情、光照和姿态），往往会牺牲图像的质量。以往的工作（如DiscoFaceGAN和3D-FM GAN）与无条件的StyleGAN相比，显示出显著的FID差距，这表明为了控制性而支付了“质量税”。作者挑战了质量与控制性不能共存的假设。

### 方法：
- 作者首先数学上形式化了3D Morphable Models（3DMM）条件面部生成问题，然后提出了一个框架，通过该框架下的简单解决方案来解决以往问题。
- 通过使用可微分的3DMM渲染器，可以直接最小化3DMM参数分布与给定这些3DMM参数的图像分布之间的互信息。
- 作者在StyleGAN2基础上训练模型，得到的3DMM条件模型在FID得分上显著优于两种最先进的方法，并且达到了与基线无条件StyleGAN2相当的值。

### 实验：
- 作者在多个方面评估了模型性能，包括生成质量、语义解耦编辑和真实图像反演。
- 使用FFHQ数据集进行训练，通过与StyleGAN2和两种基于3DMM的生成模型（DiscoFaceGAN和3D-FM GAN）进行比较，展示了模型在保持高保真度的同时，实现了对3DMM属性的有效控制。
- 通过定量比较，作者的模型在FID、精确度和召回率等指标上优于基线模型，并且在身份、表情和姿态控制方面的解耦得分也更高。
- 通过消融研究，作者进一步探讨了条件判别器、一致性损失和特征注入等不同组件对模型性能的影响。

### 结论：
- 作者提出了一个简单的条件模型，该模型基于3DMM条件面部生成的数学框架，展示了在质量和可控性方面的强性能，减少了在这两者之间选择的必要性，使控制变得“免税”。
- 作者还指出，尽管模型在图像编辑方面并非专门设计，但未来的工作可以考虑将图像编辑技术应用于该模型，以实现更好的面部编辑。

论文还讨论了实现细节、训练目标、模型架构和训练细节，并提供了项目网页的链接。


# 3DGS 

## HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior

## Motivation and Task

a novel approach for animatable human avatar generation from monocular input videos.

trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering

SMPL-X mesh


## SplatFace: Gaussian Splat Face Reconstruction Leveraging an Optimizable Surface

![[picture/Pasted image 20240511164619.png]]


**动机（Motivation）** 
SplatFace的开发旨在解决一个挑战：如何从有限数量的输入图像中重建3D人脸模型，而无需依赖于预先确定的精确几何信息。传统方法通常需要多视图图像和专业设备，尽管近期的方法有所进步，但它们仍然需要大量的输入图像或强大的几何引导。研究者的目标是通过引入一种新方法来扩大面部重建的应用范围，这种方法只需要少量输入图像，不需要精确的深度信息。

**贡献（Contributions）：**

1. 提出了一种名为SplatFace的新型高斯溅射框架，用于3D人脸重建。
2. 引入了一种联合优化策略，通过非刚性对齐过程同时优化高斯函数和可变形表面。
3. 提出了一种新的距离度量——斑点到表面距离，通过考虑高斯中心及其协方差来改善对齐。
4. 利用表面信息引入了一种世界空间密集化过程，通过利用表面信息提高重建质量。
5. 通过实验分析证明，SplatFace在新视角合成方面与其他技术具有竞争力，并且在生成具有高几何精度的3D面部网格方面也优于其他3D重建方法。

**方法（Method）：**

- **高斯溅射（Gaussian Splatting）**：将3D场景表示为一组具有自适应协方差的3D高斯原语，使用体积溅射进行渲染。
- **高斯斑点和几何表面的联合优化**：整合了一个通用的3D可变形模型（3DMM）来提供几何引导，允许仅使用有限的输入图像集进行重建。
- **斑点到表面距离（Splat-to-Surface Distance）**：引入了一个新的度量项，旨在最小化斑点和表面之间的距离函数，鼓励更好的对齐。
- **世界空间密集化（World-space Densification）**：使用表面模型增强视图空间密集化，解决高斯溅射中的浮动斑点问题。
- **组合损失函数（Combined Loss Function）**：定义了一个总损失函数，包括RGB一致性、斑点到表面距离和正则化项，以防止过拟合。

**实验（Experiments）：**

- **数据集（Datasets）**：使用FaceScape和ILSH数据集进行评估，这些数据集包含了从多个身份捕获的高分辨率光舞台图像。
- **实现（Implementation）**：在Nvidia RTX 3080上训练该方法10,000次迭代，每次运行大约需要10分钟。
- **新视角合成比较（Comparison on Novel View Synthesis）**：与3DGS、Mip-Splatting和FSGS等方法进行了定性和定量比较，结果表明SplatFace在新视角合成中产生的伪影更少，保真度更高。
- **表面网格重建比较（Comparison on Surface Mesh Reconstruction）**：将SplatFace与MVFNet、DFNRMVS、INORig和HRN等最先进的多视图3D面部重建方法进行了比较，证明了其在几何误差指标方面的优越性能。
- **消融研究（Ablation Study）**：评估了联合优化、表面初始化、斑点到表面距离和世界空间密集化的有效性，确认了它们对SplatFace方法整体性能的单独和联合贡献。


通过广泛的实验验证，SplatFace方法在仅使用少量输入图像的情况下，产生高质量的新视角渲染和精确的3D网格重建方面显示出了其有效性。

- 无geometry实验


## Gaussian Splatting Decoder for 3D-aware Generative Adversarial Networks

A combination of SR-Stylegan-Nerf-3DGS

## Drivable 3D Gaussian Avatars


## Relight head GS
这篇论文的标题是《Relightable Gaussian Codec Avatars》，作者们来自 Meta 的 Codec Avatars Lab。以下是对论文动机、方法和实验的简要中文概述：

### 动机：
- 论文旨在解决实时渲染具有高保真度的可重光照（Relightable）3D头像的挑战。这在游戏和远程通信等场景中非常重要，因为人类的面部外观对于光照变化非常敏感，需要精确模拟。
- 现有的方法在处理复杂的面部材质（如皮肤、头发和眼睛）和动态几何形状（如面部表情）时存在限制，且往往无法实时渲染高分辨率连续环境。

### 方法：
- 提出了一种名为“Relightable Gaussian Codec Avatars”的方法，使用3D高斯（Gaussians）和可学习的辐射度传输（radiance transfer）来捕捉高频细节，如头发丝和毛孔，以实现动态面部序列的3D一致性。
- 引入了一种新的可重光照外观模型，基于可学习的辐射度传输，支持人头的多样材质，并使用全局光照感知的球谐函数（spherical harmonics）处理漫反射分量，以及使用球面高斯函数处理镜面反射。
- 通过这种方式，实现了在点光源和连续光照下都能高效重光照，同时提高了眼睛反射的真实性，并允许明确的凝视控制。

### 实验：
- 通过量化评估和定性结果，展示了所提出方法在不同种族、性别和发型的多个主题上的应用。
- 使用了约144,000帧的多视角视频数据进行训练，其中包括不同的表情、句子和凝视动作。
- 在评估过程中，使用了留出的测试集，包括约9000帧的对话表情和约100帧的厌恶表情，以及10种独特的前向偏置光照模式。
- 报告了峰值信噪比（PSNR）、结构相似性（SSIM）和感知图像质量（LPIPS）等指标，以评估模型性能。

### 结论：
- 该方法在不牺牲实时性能的前提下，提供了比以前的方法更高的重光照质量，特别是在处理头发、皮肤和眼睛的高频光照细节方面。
- 论文还讨论了当前方法的局限性和未来的研究方向，如去除对粗略网格和凝视跟踪的依赖，以及扩展到野外输入的挑战。

这篇论文的核心贡献在于提出了一种新的3D头像表示方法，能够在实时渲染环境中实现高保真的重光照效果，这对于虚拟角色的创建和动画制作具有重要意义。

## MonoGaussion
这篇论文的标题是《MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar》，作者们分别来自哈尔滨工业大学、清华大学和中国传媒大学。以下是对论文动机、方法和实验的简要中文概述：

### 动机：
- 论文旨在解决如何从单目肖像视频序列中动画化逼真的3D头部头像的问题，这是虚拟与现实世界之间的重要桥梁。
- 现有的3D头像技术，包括3D形态可变模型（3DMM）、点云和神经隐式表示，存在一些限制，如固定拓扑结构、训练负担重、变形灵活性和渲染效率有限。

### 方法：
- 提出了MonoGaussianAvatar，这是一种新的方法，使用3D高斯点表示和高斯变形场来从单目肖像视频中学习显式的头部头像。
- 通过定义具有可适应形状的高斯点来表征头部头像，这些点能够灵活地改变拓扑结构，并且随着高斯变形场的运动而移动，与目标姿态和表情对齐，从而实现高效的变形。
- 高斯点还具有可控的形状、大小、颜色和不透明度，结合高斯溅射技术，允许高效的训练和渲染。

### 实验：
- 通过实验展示了该方法的优越性能，与先前的方法相比，在结构相似性、图像相似性和峰值信噪比（PSNR）等指标上达到了最先进的结果。
- 使用了从IMavatar、Nerface和使用智能手机捕获的主体的数据集进行比较。
- 与三种最先进的面部重演方法进行了比较，包括Nerface、IMavatar和PointAvatar，并展示了在自驱动重动画和面部重演方面的优越性能。
- 还进行了消融实验，探讨了高斯参数变形、点插入和删除策略以及点的初始设置对模型性能的影响。

### 结论：
- MonoGaussianAvatar是一种基于高斯点的显式头像模型，它结合了面部表情和姿态，可以从单目视频中重建详细的几何结构和外观。
- 该方法在渲染质量和训练效率方面均优于现有的单目3D头部头像方法，但也有一些局限性，如无法模拟眼镜镜片的反射，以及在处理超出预定义先验范围的极端表情时可能会受到限制。

论文还讨论了技术的潜在社会影响，包括在部署前需要谨慎处理创建“深度伪造”视频的风险。作者认为他们提出的3D高斯点表示将有助于有效和高效3D头部头像表示的进展。


# Video Processgin

## End-to-End Spatio-Temporal Action Localisation with Video Transformers
作者是 Alexey Gritsenko、Xuehan Xiong、Josip Djolonga、Mostafa Dehghani、Chen Sun、Mario Lučic、Cordelia Schmid 和 Anurag Arnab，来自 Google Research。以下是对论文动机、方法和实验的简要中文概述：

### 动机：

- 论文旨在解决视频中的时空动作定位问题，这对于高级视频搜索引擎、机器人和安全等领域具有重要应用。
- 现有的性能最好的模型通常使用外部人体提案和复杂的外部记忆库。这些方法需要额外的预处理和后处理步骤，如区域建议和非极大值抑制（NMS）。

### 方法：
- 提出了一个完全端到端的、基于纯变换器的模型，该模型直接处理输入视频，并输出tubelets（一系列边界框和每帧的动作类别）。
- 该模型灵活，可以使用单个帧的稀疏边界框监督进行训练，或者使用完整的tubelet注释进行训练。在这两种情况下，它都能预测一致的tubelets作为输出。
- 与大多数先前工作不同，该模型不需要额外的预处理提案或后处理NMS。模型基于DETR（Detection Transformer）检测模型，使用变换器架构。

### 实验：
- 在四个不同的时空动作定位基准测试中进行了广泛的消融实验，并显著提高了使用稀疏关键帧和完整tubelet注释的最新结果。
- 在AVA和AVA-Kinetics数据集上实现了Frame mAP 44.6的成绩，超过了之前发表的工作8.2个百分点，并且在UCF101-24数据集上的视频AP50超过了先前工作11.6个百分点。
- 实验结果表明，该模型在不需要任何外部人体检测器提供提案、复杂的记忆库或额外的目标检测器的情况下，就能实现最先进的结果，并且模型预测的tubelets是时间上一致的。

### 结论：
- 提出的STAR（Spatio-Temporal Action Transformer）模型是一个端到端的时空动作定位模型，能够在稀疏关键帧或完整tubelet注释的情况下输出tubelets。
- 该方法在四个动作定位数据集上实现了最先进的结果，超越了使用外部提案和记忆库的复杂方法，证明了变换器骨干足以捕捉输入视频中的长期依赖关系。



论文还讨论了实现细节、训练目标、模型架构和训练细节。此外，作者还探讨了模型的潜在社会影响，包括在部署前需要谨慎处理创建“深度伪造”视频的风险，并指出了模型的一些限制。

## Vamos: Versatile Action Models for Video Understanding
作者是 Shijie Wang, Qi Zhao, Minh Quan Do, Nakul Agarwal, 和 Kwonjoon Lee，来自布朗大学和本田研究所。以下是对论文动机、方法和实验的简要中文概述：

### 动机：
- 论文探讨了什么样的视频表示对于视频理解任务（如预测未来活动或回答视频相关问题）是有效的。早期的方法侧重于直接从视频像素进行端到端学习，但作者提出重新审视基于文本的表示，例如离散的动作标签或自由形式的视频字幕，这些表示是可解释的，并且可以直接被大型语言模型（LLMs）使用。

### 方法：
- 作者提出了一个名为Vamos的灵活动作模型框架，该框架由一个大型语言模型作为“推理器”，能够灵活地利用从视频中提取的视觉嵌入、动作标签和自由形式描述作为输入。
- Vamos框架结合了分布式视觉嵌入、离散动作标签和自由形式文本描述三种表示方式，并利用大型语言模型（如Llama-2）应用于各种应用。

### 实验：
- Vamos在四个互补的视频理解基准测试中进行了评估，包括Ego4D、Next-QA、IntentQA和EgoSchema，测试其对时间动态建模、视觉历史编码和推理的能力。
- 实验结果表明，基于文本的表示在所有基准测试中都取得了竞争性的表现，而视觉嵌入提供的性能提升较小或没有提升，证明了在大型语言模型时代基于文本的视频表示的有效性。
- 作者进行了广泛的消融研究和定性分析来支持这些观察结果，并在三个基准测试中实现了最先进的性能。

### 结论：
- 论文提出了在大型语言模型时代重新审视视频表示和动作建模的方法，并探索了自由形式文本描述的有效性。
- Vamos框架通过定量实验和定性分析展示了自由形式文本作为表示的性能、可解释性，并且可以通过因果干预进行修正，即使在被大幅度压缩到原始长度的6%时仍然有效。
- Vamos在Ego4D LTA、NeXT-QA、IntentQA和EgoSchema上取得了最先进的性能，证明了文本表示在视频理解任务中的潜力。尽管如此，作者认为视觉信息对于复杂的视频理解和推理仍然是必不可少的，并呼吁对建模和基准测试进行进一步的探索。

论文还讨论了实现细节、训练目标、模型架构和训练细节，并提供了代码和模型的发布信息。

## A good picture

![[picture/Pasted image 20240511172821.png]]
