朴素贝叶斯法是基于贝叶斯定理和特征条件独立假设的分类方法。 和贝叶斯估计(Bayesian estimation)是不同的概念。 

# 0 关键概念
## 0.1 条件概率
P(X|Y)表示在给定Y的时候X的概率分布
为什么要把条件写在后面呢？ 因为英语用 given Y的话可以放在后面，符合英语的语序
## 0.2 联合概率分布
P(X,Y)表示在X=x和Y=y同时满足的情况下的概率。
参考f(x,y)表示X=x,Y=y时的函数值的记号
## 0.3 贝叶斯定理
$$
P(A|B) = \frac{P(A)P(B|A)}{P(B)}
$$

# 1 基本方法
输入空间 $\mathcal{X} \subseteq \mathcal{R}^n$,输出空间类别标签$\mathcal{Y} = \{  c_{1},  c_{2}, c_{3},\dots c_{k}\}$,X,Y分别是定义在输入空间$\mathcal{X}$,输出空间$\mathcal{Y}$上的随机向量。 $P(X,Y)$是X,Y的联合概率分布。 训练数据集$$
z
$$
由$P(X,Y)$独立同分布产生。 

贝叶斯方法由训练数据集T学习到联合概率分布$P(X,Y)$,通过学习:
1. 先验概率分布 $P(Y=c_{k})$
2. 条件概率分布 $P(X=x|Y=c_{k})=P(X^{(1)}=x^{(1)},\dots,^{(n)}=x^{(n)}|Y=c_{k})$
3. 联合概率分布$P(X,Y)$

由朴素贝叶斯假设条件独立性:
$$\begin{align}
P(X=x|Y=c_{k})&=P(X^{(1)}=x^{(1)},\dots,^{(n)}=x^{(n)}|Y=c_{k})\\   \\
&= \prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_{k})

\end{align}$$

分类时，给定输入x，计算$y=c_k$的后验概率，并且将概率最大的$c_{k}$作为输出，由贝叶斯公式可得:
$$
\begin{align}
P(Y=c_{k}|X=x) &= \frac{P(X=x|Y=c_{k})P(Y=c_{k})}{P(X=x)}\\ \\
\end{align}
$$
其中$$
P(X=x) = \sum P(X=x|y=c_{k})P(y=c_{k})
$$
带入上式，所以得到:
$$
\begin{align}
P(Y=c_{k}|X=x) &= \frac{P(X=x|Y=c_{k})P(Y=c_{k})}{P(X=x)}\\ 
&= \frac{P(X=x|Y=c_{k})P(Y=c_{k})}{\sum P(X=x|y=c_{k})P(y=c_{k})} \\ \\
&=\frac{P(Y=c_{k}\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_{k}))}{\sum P(y=c_{k})\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_{k})} \\
\end{align}
$$
所以预测结果可以表示为:
$$
y_{pred} = f(x) = argmax_{c_{k}}\frac{P(Y=c_{k}\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_{k}))}{\sum P(y=c_{k})\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_{k})}
$$
注意到分母对不同的c$_k$取值相同，所以不影响预测概率的排序:
$$y_{pred} = argmax_{c_{k}}{P(Y=c_{k}\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_{k}))}$$


# 2 期望风险最小化与后验概率最大化
设0-1选择损失函数
$$
L(Y,f(X)) =  \begin{cases}1, &  Y= f(X), \\ 0, & Y\neq f(X) \end{cases}
$$
f为分类函数
期望风险函数为
$$
\begin{align}
R_{\exp}(f)&=\mathbb{E}[L(Y,f(X))]\\ 
 & =\sum_{x} \sum_{y} P(x,y) L(Y,f(X)) \\ 
&= \sum_{x} [P(x) \sum_{y} P(y|x) L(Y,f(X))] \\
&= \mathbb{E}_{x}[\sum_{y} P(y|x) L(Y,f(X))]\\
&= \mathbb{E}_{x}\sum_{k=1}^K [L(c_{k},f(X))]P(c_{k}|X)\\
\end{align}
$$
对期望风险最小化，就需要对X=x逐个最小化
$$
\begin{align}
f(x) &= argmin_{y} \sum_{k=1}^K [L(c_{k},f(X))]P(c_{k}|X)\\ 
&= argmin_{y} \sum_{k=1}^K P(c_{k}\neq y|X=x)\\ 
&= argmin_{y}1-P(c_{k}= y|X=x)\\ 
&= argmax_{y} P(c_{k}= y|X=x)\\
\end{align}
$$
所以得到经验风险最小化和后验概率最大化一致。 


# 3 朴素贝叶斯的参数估计

## 3.1 极大似然估计
在朴素贝叶斯法中，学习意味着人估计$P(Y=c_{k})$和$P(X^{j}=x^{j}|Y=c_{k})$。 可以应用极大似然估计法估计相应概率:

$$P(Y=c_{k})=\frac{\sum_{i=1}^NI(y_{i}=c_{k})}{N},k=1,2,3,\dots,K$$

即计算每个类别出现的频率。 

$$
P(X^{j}=x^{j}|Y=c_{k})=\frac{\sum_{i=1}^N I(x_{i}^j=a_{jl},y_{i}=c_{k})}{\sum_{i=0}^{N}{I(y_{i}=c_{k})}},j=1,2,3,..n;l=1,2,3,\dots,S_{j},k=1,2,3,\dots,K
$$

j表示x的第j个维度，l表示维度j的第l个取值，k表示第k类。 即计算每个类别k中的样本x在j维度上

# 习题
